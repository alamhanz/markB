{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Projector\n",
    "\n",
    "[source](https://stackoverflow.com/questions/57014236/how-to-use-the-embedding-projector-in-tensorflow-2-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "import tensorboard\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir='../../artifact/projector/'\n",
    "\n",
    "PATH_INTERIM = '../../data/interim/' \n",
    "PATH_MODEL = '../../model/'\n",
    "# PATH_LogDir = '../../model/projectors/'\n",
    "PATH_LogDir = '../../model/multi_projectors/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard.__version__\n",
    "\n",
    "## '2.5.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket = pd.read_csv(PATH_INTERIM + 'all_basket2.csv')\n",
    "all_items = list(set((','.join(df_basket['norms'])).split(',')))\n",
    "all_items.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate The Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = [\"w2v_50_3_1.model\",\"w2v_100_3_1.model\",\"w2v_50_3_2.model\",\"w2v_100_3_2.model\"]\n",
    "\n",
    "all_v_list = []\n",
    "all_metadata = []\n",
    "len(all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs_temp = embs[0]\n",
    "for embs_temp in embs :\n",
    "    w2v_temp = Word2Vec.load(PATH_MODEL+embs_temp)\n",
    "    all_v = []\n",
    "    meta_items = []\n",
    "    for word in all_items :\n",
    "        try :\n",
    "            vec = w2v_temp.wv[word]\n",
    "            all_v.append(vec)\n",
    "            meta_items.append(word) \n",
    "           \n",
    "           \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    all_v = np.array(all_v)\n",
    "\n",
    "    all_v_list.append(all_v)\n",
    "    all_metadata.append(meta_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((211, 150), (211, 150))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_v_list[0].shape,all_v_list[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, 211)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_metadata[0]),len(all_metadata[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_meta2 = df_meta[['#','Name']]\n",
    "# df_meta2[:100].to_csv(log_dir + 'meta_mini.tsv', sep=\"\\t\")\n",
    "# df[:100].to_csv(log_dir + 'vecs_mini.tsv', sep=\"\\t\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta0.tsv: 211\n",
      "meta1.tsv: 211\n",
      "meta2.tsv: 259\n",
      "meta3.tsv: 259\n",
      "WARNING:tensorflow:Saver is deprecated, please switch to tf.train.Checkpoint or tf.keras.Model.save_weights for training checkpoints. When executing eagerly variables do not necessarily have unique names, and so the variable.name-based lookups Saver performs are error-prone.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../model/multi_projectors/all_embeddings.cpkt-0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feats = []\n",
    "for i in range(len(embs)):\n",
    "    \n",
    "    metadata_used = all_metadata[i]\n",
    "    vector_used = all_v_list[i]\n",
    "    \n",
    "    metadata_file = 'meta'+str(i)+'.tsv'\n",
    "#     vector_file = 'embed'+str(i)+'.ckpt'\n",
    "\n",
    "    ## Metadata\n",
    "    # pd.DataFrame(metadata_used).to_csv(PATH_LogDir + metadata_file, sep=\"\\t\", index = False)\n",
    "    out_m = io.open(PATH_LogDir+metadata_file, 'w', encoding='utf-8')\n",
    "    for word in metadata_used:\n",
    "        out_m.write(word + \"\\n\")\n",
    "    out_m.close()\n",
    "    print(metadata_file+':',len(metadata_used))\n",
    "\n",
    "    ## Vector\n",
    "    we = tf.Variable(vector_used, trainable=False,name=('embedM_'+str(i)))\n",
    "    # we = tf.Variable(vector_used, trainable=False)\n",
    "    \n",
    "    # vector_file = we.name[:-2]+'.ckpt'\n",
    "    # checkpoint = tf.train.Checkpoint(embedding=we)\n",
    "    # checkpoint.save(PATH_LogDir + vector_file)\n",
    "    all_feats.append(we)\n",
    "    \n",
    "saver = tf.compat.v1.train.Saver(all_feats)  # Must pass list or dict\n",
    "saver.save(sess=None, global_step=0, save_path=PATH_LogDir+'all_embeddings.cpkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = tf.train.load_checkpoint(PATH_LogDir)\n",
    "map = reader.get_variable_to_shape_map()\n",
    "key_to_use = \"\"\n",
    "for key in map:\n",
    "    if \"embeddings\" in key:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedM_0': [211, 150],\n",
       " 'embedM_1': [211, 150],\n",
       " 'embedM_2': [259, 150],\n",
       " 'embedM_3': [259, 150]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.get_variable_to_shape_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.summary.create_file_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "config = projector.ProjectorConfig()\n",
    "for wv0 in all_feats:\n",
    "    metadata_file = 'meta'+str(k)+'.tsv'\n",
    "    \n",
    "    # Set up config.\n",
    "    \n",
    "    embedding = config.embeddings.add()\n",
    "    # The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "    # embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "    # embedding.tensor_name = \"embedding_15k_v7.cpkt\"\n",
    "    embedding.tensor_name = wv0.name[:-2]\n",
    "    # embedding.tensor_path = wv0.name[:-2]\n",
    "    embedding.metadata_path = metadata_file\n",
    "    k += 1\n",
    "projector.visualize_embeddings(PATH_LogDir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embeddings {\n",
       "  tensor_name: \"embedM_0\"\n",
       "  metadata_path: \"meta0.tsv\"\n",
       "}\n",
       "embeddings {\n",
       "  tensor_name: \"embedM_1\"\n",
       "  metadata_path: \"meta1.tsv\"\n",
       "}\n",
       "embeddings {\n",
       "  tensor_name: \"embedM_2\"\n",
       "  metadata_path: \"meta2.tsv\"\n",
       "}\n",
       "embeddings {\n",
       "  tensor_name: \"embedM_3\"\n",
       "  metadata_path: \"meta3.tsv\"\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir model/multi_projectors\n",
    "# %tensorboard --logdir ../../model/projectors\n",
    "# !kill 17424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[word2vec-embeddings-in-tensorboard](https://stackoverflow.com/questions/50492676/visualize-gensim-word2vec-embeddings-in-tensorboard-projector)\n",
    "\n",
    "[projector_plugin](https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboard==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/45020971/visualizing-multiple-embedding-with-tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "droid1",
   "language": "python",
   "name": "droid1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
